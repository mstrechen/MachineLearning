{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classifier (least squares method)\n",
    "\n",
    "This is simple classifier based on [least squares method](https://en.wikipedia.org/wiki/Least_squares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import numpy and time libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's read the data. It is written in two files (<i>mnist_train.csv</i> and <i>mnist_test.csv</i>) picked from [here](https://pjreddie.com/projects/mnist-in-csv/). \n",
    "\n",
    "<b>Attention!</b> Reading and parsing big csv files needs some memory (up to a couple of gigabytes) and some calculation power, so it's better to run the following block of code only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=',')\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 60k vectors representing images in `train_data` and 10k vectors in `test_data`. Every vector has 785 values - first one is the tag (a digit from 0 to 9) and next 784 numbers represent the image of this digit (actually it is reshaped matrix of pixels 28x28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start. Our algorithm has complexity of $O(N \\times M \\times L)$, where $N$ is the size of train selection, $M$ is the size of test selection and $L$ represents the length of vectors in selections. Therefore, we need to choose  subsets of our selections and  work with it, no with the full data. \n",
    "\n",
    "We still can take full data, but then computations will require a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = 10000\n",
    "test_N = 1000\n",
    "IMAGE_LENGTH = test_data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_subset(data, new_size):\n",
    "    old_size = data.shape[0]\n",
    "    indexes = np.random.choice(old_size, new_size, replace = False)\n",
    "    labels = data[indexes, 0].astype(int)\n",
    "    images = data[indexes, 1:]\n",
    "    return labels, images\n",
    "\n",
    "train_labels, train_img = get_random_subset(train_data, train_N)\n",
    "test_labels, test_img = get_random_subset(test_data, test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write our classifier itself. As said above, we will use the least squares method. \n",
    "\n",
    "How does it work?\n",
    "For every sample from test selection (which is a vector), we need to find the closest (by Euclidean distance) vector from train selection. The label of the closest vector will be the \"predicted\" label of vector from test selection.\n",
    "\n",
    "\n",
    "<b>Comment:</b> for small sizes of test and train subsets we could vectorize our algorithm by creating matrix of Euclidean distance between each pair of vectors. However, it will rise memory usage which is critical for huge subsets of images.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(test_img, train_img, train_labels):\n",
    "    DM = np.square(test_img - train_img).sum(axis = 1)\n",
    "    index = DM.argmin(axis = 0)\n",
    "    return train_labels[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our algorithm. We will also measure time of execution using `time` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLES COUNT : 10000\n",
      "TESTS COUNT   : 1000\n",
      "ACCURACY      : 94.4 %\n",
      "CPU times: user 1min 21s, sys: 48.8 s, total: 2min 10s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predicted_results = [classify_image(test, train_img, train_labels) \n",
    "                     for test in test_img]\n",
    "\n",
    "success_count = (predicted_results == test_labels).sum()\n",
    "accuracy = success_count / test_N\n",
    "\n",
    "print(\"SAMPLES COUNT :\", train_N)\n",
    "print(\"TESTS COUNT   :\", test_N)\n",
    "print(\"ACCURACY      :\", np.round(accuracy * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done!\n",
    "\n",
    "You can see how accuracy depends on size of train set by changing `train_N` and restarting code by your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MachineLearning]",
   "language": "python",
   "name": "conda-env-MachineLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

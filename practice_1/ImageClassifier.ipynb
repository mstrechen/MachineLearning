{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classifier (least squares method)\n",
    "\n",
    "This is simple classifier based on [least squares method](https://en.wikipedia.org/wiki/Least_squares)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's import numpy and read data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is written in two files (<i>mnist_train.csv</i> and <i>mnist_test.csv</i>) picked from [here](https://pjreddie.com/projects/mnist-in-csv/). \n",
    "\n",
    "<b>Attention!</b> Reading and parsing big csv files needs some memory (up to a couple of gigabytes) and some calculation power, so it's better to run the following block of code only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt(\"mnist_train.csv\", delimiter=',')\n",
    "test_data = np.loadtxt(\"mnist_test.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 60k vectors representing images in `train_data` and 10k vectors in `test_data`. Every vector has 785 values - first one is the tag (a digit from 0 to 9) and next 784 numbers represent the image of this digit (actually it is reshaped matrix of pixels 28x28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n",
      "(10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's start. Our algorithm has complexity of $O(N \\times M \\times L)$, where $N$ is the size of train selection, $M$ is the size of test selection and $L$ represents the length of vectors in selections. Therefore, we need to choose  subsets of our selections and  work with it, no with the full data. \n",
    "\n",
    "We still can take full data, but then computations will require a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = 10000\n",
    "test_N = 1000\n",
    "IMAGE_LENGTH = test_data.shape[1] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_subset(data, new_size):\n",
    "    old_size = data.shape[0]\n",
    "    indexes = np.random.choice(old_size, new_size, replace = False)\n",
    "    labels = data[indexes, 0].astype(int)\n",
    "    images = data[indexes, 1:]\n",
    "    return labels, images\n",
    "\n",
    "train_labels, train_img = get_random_subset(train_data, train_N)\n",
    "test_labels, test_img = get_random_subset(test_data, test_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write our classifier itself. As said above, we will use the least squares method. \n",
    "\n",
    "How does it work?\n",
    "For every sample from test selection (which is a vector), we need to find the closest (by Euclidean distance) vector from train selection. The label of the closest vector will be the \"predicted\" label of vector from test selection.\n",
    "\n",
    "\n",
    "\n",
    "<b>Comment:</b> our algorithm creates matrix of size $N \\times M \\times L$, where $N$ is size of train selection, $M$ is size of test selection and $L$ is the length of vectors. So, we need $O(N \\times M \\times L)$ memory (about $10^4 \\cdot 6 \\cdot 10^4 \\cdot 784 \\cdot SizeOfFloat \\approx 1,9 \\cdot 10^{12}$ bytes of memory for maximal size of train and test data and we cant handle so much). Therefore, it's better to process test data one-by-one. It will rise time usage (not so much, actually), but will reduce memory usage. Space-time trade-off as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_images(train_img, train_labels, test_img, N_train, N_test):\n",
    "    Tr = train_img.reshape(N_train, 1, IMAGE_LENGTH)\n",
    "    Te = test_img.reshape(1, N_test, IMAGE_LENGTH)\n",
    "    DM = np.square(Te - Tr).sum(axis = 2)\n",
    "    indexes = DM.argmin(axis = 0)\n",
    "    return train_labels[indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our algorithm. We will also measure time of execution using `time` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLES COUNT : 10000\n",
      "TESTS COUNT   : 1000\n",
      "ELAPSED TIME  : 58.95 seconds\n",
      "ACCURACY      : 95.2 %\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "success_count = 0\n",
    "\n",
    "begin_time = time.time()\n",
    "\n",
    "for label, test in zip(test_labels, test_img):\n",
    "    res = classify_images(train_img, train_labels, test, train_N, 1)\n",
    "    if(res[0] == label):\n",
    "        success_count += 1\n",
    "\n",
    "end_time = time.time()\n",
    "        \n",
    "accuracy = success_count / test_N\n",
    "\n",
    "print(\"SAMPLES COUNT :\", train_N)\n",
    "print(\"TESTS COUNT   :\", test_N)\n",
    "print(\"ELAPSED TIME  :\", np.round(end_time - begin_time, 2), \"seconds\")\n",
    "print(\"ACCURACY      :\", np.round(accuracy * 100, 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done!\n",
    "\n",
    "You can see how accuracy depends on size of train set by changing `train_N` and restarting code by your own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MachineLearning]",
   "language": "python",
   "name": "conda-env-MachineLearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
